---
title: "Modeling California Housing Markets using Multiple Linear Regression"
author: "Joshua Zhong"
date: "12/28/2023"
output:
 html_document:
  toc: yes
  toc_depth: 4
  toc_float: yes
  fig_width: 6
  fig_caption: yes
  number_sections: yes
  theme: readable
  editor_options:
  chunk_output_type: console
  code_folding: hide
---



```{=html}

<style type="text/css">

/* Cascading Style Sheets (CSS) is a stylesheet language used to describe the presentation of a document written in HTML or XML. it is a simple mechanism for adding style (e.g., fonts, colors, spacing) to Web documents. */

h1.title {  /* Title - font specifications of the report title */
  font-size: 24px;
  color: DarkRed;
  text-align: center;
  font-family: "Gill Sans", sans-serif;
}
h4.author { /* Header 4 - font specifications for authors  */
  font-size: 20px;
  font-family: system-ui;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - font specifications for the date  */
  font-size: 18px;
  font-family: system-ui;
  color: DarkBlue;
  text-align: center;
}
h1 { /* Header 1 - font specifications for level 1 section title  */
    font-size: 22px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}
h2 { /* Header 2 - font specifications for level 2 section title */
    font-size: 20px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { /* Header 3 - font specifications of level 3 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - font specifications of level 4 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

body { background-color:white; }

.highlightme { background-color:yellow; }

p { background-color:white; }

</style>
```

```{r setup, include=FALSE}

   cahouses = read.csv("https://raw.githubusercontent.com/JZhong01/STA321/main/Topic%203/ca-housing-price.csv", 
   header = TRUE)
   
  options(scipen = 2)

   library(ggplot2)  
   library(knitr)
   library(leaflet)
   library(EnvStats)
   library(MASS)
   library(phytools)
   library(boot)
   library(psych)
   library(car)
   library(dplyr)
   library(kableExtra)

# Specifications of outputs of code in code chunks
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	results = TRUE, 
	collapse = TRUE,
	digits = 4
)
   
```

# Abstract

This statistical report explores the dynamics of California house prices, utilizing a modified version of the California Housing data set derived from the 1990 census data. Employing methodologies such as K-means clustering, Box-Cox transformations, and bootstrapping, this analysis aims to uncover nuanced patterns and relationships within this housing market.

Key predictors that were analyzed spanned geographical location, housing characteristics, demographic details, and economic indicators of US Census block groups (i.e. neighborhoods grouped by the Census Bureau that will act as our observational unit ) and their inhabitants. 

The study begins with exploratory data analysis, employing Box-Cox transformations to address issues of non-normality and heteroscedasticity in the data set. Subsequently, a robust bootstrapping approach is applied across cases then residuals, offering a comprehensive assessment given our inability to make assumptions about the underlying distribution. Through these techniques, we seek to provide a more accurate understanding of the factors influencing house prices across California's diverse census block groups.

# Introduction

## Motivation

The motivation behind undertaking this comprehensive data analysis of California house prices lies in the quest to unravel the intricate dynamics influencing one of the most pivotal aspects of the state's economy – its real estate market. As housing affordability and market trends continue to be of paramount importance for residents, policymakers, and real estate professionals alike, there exists a compelling need to delve into the underlying factors that shape housing prices across diverse census block groups.

This analysis seeks to provide actionable insights for informed decision-making in various spheres. By applying advanced statistical techniques such as Box-Cox transformations and bootstrapping, we aim to not only mitigate data challenges but also enhance the robustness and accuracy of our findings. The significance of this study is amplified by its foundation in the 1990 California census data, offering a historical perspective that enriches our understanding of how socio-economic and spatial factors have shaped the housing landscape over time.

## Data set Description

The data set was found on Kaggle. It includes information gathered from various block groups in California during the 1990 Census. The U.S. Census Bureau uses the block group as the smallest geographical unit, typically consisting of 600 to 3000 people. Each observation in the data set represents one block group and comprises an average of 1425.5 individuals in a geographically compact area. 

The data set contains 20640 observations of 9 dependent variables and one independent variable: median house value. The link to the raw CSV file in posted on GitHub:  https://raw.githubusercontent.com/JZhong01/STA321/main/Topic%203/ca-housing-price.csv.

- Longitude(X1): How far west block group is located from prime meridian
- Latitude(X2): How far north block group is located from equator
- HousingMedianAge(X3): Median age of house within block group
- TotalRooms(X4): Total number of rooms in block 
- TotalBedrooms(X5): Total number of bedrooms in block
- Population(X6): Total number of residents in block group
- Households(X7): Total number of households, defined as a group of people residing within a home unit
- MedianIncome(X8): Median income, in thousands of US Dollars, for households within a block group
- MedianHouseValue(Y): Median house value, in US Dollars, in a block
- OceanProximity(X9): Location of the house to an ocean or sea, displayed as a categorical variable


## Research Question

The primary objective of this analysis is to identify the primary factors that influence house prices amongst California census block groups. 


Practical questions that this research aims to explore are as follows: 

- What socioeconomic factors exhibit the largest correlation with median house price?
- What transformations may need to be conducted on the data, or do no transformations need to be done?


This analysis will seek to explore 2 hypotheses: 

- Hypothesis 1: A reduced model will perform better than the full model. 

- Hypothesis 2: Distance to the ocean is negatively correlated with median house value.

# Methods

This section outlines the key methodologies employed in the analysis, including multiple linear regression, K-means clustering, Box-Cox transformation, and bootstrapping. Each method is designed to address specific research goals and improve model robustness.

## Multiple Linear Regression

Multiple linear regression (MLR) serves as the foundational model for examining how multiple predictors, such as housing characteristics and demographic factors, influence California house prices. In this analysis, MLR allows us to quantify the relationship between the dependent variable, MedianHouseValue, and several independent variables including median income, population, and geographic location. The initial model will include all predictors to provide a baseline understanding before refining the model based on diagnostics. Key assumptions include linearity of relationships, independence of observations, homoscedasticity (constant variance of residuals), and normally distributed residuals.

## K-means Clustering

To capture the geographical variation of latitude and longitude in a more interpretable and meaningful way, K-means clustering is employed. This method groups California’s block groups into distinct geographic regions, which are used as a new categorical variable, GeoCluster. This clustering technique allows for latitude and longitude to be useful predictors, as considering them as separate predictors is unreliable. An elbow plot is employed to determine the optimal number of clusters. The main assumption is that geographic proximity (as measured by distance) meaningfully influences house prices, and that the number of clusters adequately represents regional differences.

## Box-Cox Transformation

Box-Cox transformation is employed to address violations of regression assumptions, particularly non-normality and heteroscedasticity of residuals. By transforming the dependent variable, MedianHouseValue, the Box-Cox method helps stabilize variance and approximate a normal distribution of residuals, improving the overall fit of the regression model. This transformation assumes that the relationship between the predictors and the dependent variable can be linearized and that the response variable is strictly positive, which holds true in this dataset.

## Bootstrap Residuals

Bootstrapping is used to generate robust confidence intervals for the regression coefficients, especially when the assumptions of normality for residuals are violated. By resampling the dataset 1,000 times with replacement, we estimate the variability in the coefficients without relying on parametric assumptions. Bootstrapping is a flexible, non-parametric approach, with the key assumption being that the sample is representative of the population, and resampling can approximate the true sampling distribution of the model estimates.

## Other Methods

In addition to the core methods listed above, other statistical techniques may be employed, depending on the results of the initial analyses:

1) Variance Inflation Factor (VIF): This will be used to detect multicollinearity in the multiple regression models.

2) Residual Diagnostics: Standard diagnostic plots (Q-Q plot, residuals vs. fitted, etc.) will be used to assess the validity of assumptions in the regression model.

3) Cross-Validation: If beneficial, cross-validation may be used to validate the robustness of the final model and prevent overfitting.


# Exploratory Data Analysis

The original data set contains 9 independent variables and 1 dependent variable. Before proceeding with model building, it is important to explore the relationships among the variables, identify any patterns, and address potential issues like multicollinearity or irrelevant variables.


## Data Characteristics

These data were originally published in the 1997 paper titled *Sparse Spatial Autoregressions* by Pace et al. Each observation represents one census block group from the 1990 US Census conducted in California. 

There is only one response variable - the median house value of the block group. There are 9 predictor variables consisting of 8 numerical variables and one nominal variable. These numerical variables describe geographic location (longitude, latitude), house characteristics (median house age, total rooms, total bedrooms), resident characteristics (population, households), and socioeconomic characteristics (median household income). The categorical variable very roughly describes 5 categories of relation to the ocean (<1 hour away, inland, near ocean, near bay, or island). The summary statistics are as follows: 

<br>

```{r summary of variables}

cahouses %>%
  summary() %>%
  t() %>%
  kable(format = "markdown", caption = "Summary of Data Set's Statistics", )


```

### Handling Missing Values

There are 207 missing values for total_bedrooms. As such, these observations will be removed. While it's normally not advised to remove observations due to concerns of losing information from the data, there are 20640 observations, meaning that removing 207 observations will have a very minor impact. The benefits of removing the observations with missing values outweigh the drawbacks.  

```{r remove NAs}
  cahouses <- na.omit(cahouses, cols = "total_bedrooms")
  sum(is.na(cahouses))
```


### Standardizing Income

There is a discrepancy in units for median income of households and median house value. The units for median income are in tens of thousands of US Dollars whereas the units for median house value are in US Dollars. As such, median income is multiplied by a factor of 10000 so that the units are standardized to US Dollars. 

```{r convert median income}
# conversion of income to USD
  cahouses$median_income <- cahouses$median_income * 10000
  summary(cahouses$median_income)
```



## Geographic Factors

Longitude and latitude are presented in the data set as 2 separate, standalone variables. However, they should not be used in this way because they do not inherently convey meaningful geographic information when split. Instead, exploratory analysis is conducted to detect whether any patterns occur when both longitude and latitude are used in conjunction. This information can then be used to derive a new categorical variable.

First, a graph of observations' longitude and latitude are plotted to see if patterns emerge.

```{r long and lat}

ggplot(cahouses, aes(x = longitude, y = latitude)) +
  geom_point(alpha = 0.4, color = 'blue') +
  labs(title = "Scatter Plot of Longitude and Latitude for California Housing Data", 
       x = "Longitude", y = "Latitude") +
  theme_minimal()

```

This scatter plot helps us visualize our geographic data. Based on my observations, the housing data could be sorted into clusters that represent what part of California best 

### Creating Geographic Clusters

Before clustering longitude and latitude, we'll create an elbow plot to help determine the optimal number of clusters.

The plot graphs the total within-cluster sum of squares (WCSS), which measures the compactness of the clusters. As the number of clusters increases, the WCSS decreases because the data points are closer to the centroids of their respective clusters. However, after a certain point (the "elbow"), adding more clusters results in only marginal improvements, indicating that this is the optimal number of clusters. Finding this elbow allows for us to choose a K that appropriately balances model complexity and cluster compactness. 


```{r elbow plot}

# Subset the data to include only latitude and longitude
geo_data <- cahouses[, c('longitude', 'latitude')]

# Set a range of potential K values (number of clusters)
wcss <- numeric()  # Initialize an empty vector for within-cluster sum of squares

# Calculate WCSS for different numbers of clusters
for (k in 1:10) {
  kmeans_result <- kmeans(geo_data, centers = k)
  wcss[k] <- sum(kmeans_result$tot.withinss)  # Store the WCSS for each K
}

# Create the elbow plot
elbow_plot <- data.frame(K = 1:10, WCSS = wcss)

ggplot(elbow_plot, aes(x = K, y = WCSS)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 3) +
  labs(title = "Elbow Plot for Optimal Number of Clusters", 
       x = "Number of Clusters (K)", 
       y = "Within-Cluster Sum of Squares (WCSS)") +
  scale_x_continuous(breaks = seq(1, 10, by = 1)) +  # Ensure whole numbers on x-axis
  ylim(0, 50000) +  # Adjust to fit WCSS scale better
  theme_minimal()


```

The first cluster in the elbow plot was not provided because it has a WCSS of over 175000. In addition, while there would be a notable drop off in WCSS from k=1 to k=2 as compared to k=2 to k=3, choosing k=2 clusters may be an oversimplification and may mask smaller, yet meaningful geographic variation. 

As a result, the elbow plot determines that k=4 clusters will be used for the K-means Clustering. 


```{r categorize geo regions}

# Clustering longitude and latitude into regions using k-means
set.seed(123) # Setting a seed for reproducibility
kmeans_result <- kmeans(cahouses[, c('longitude', 'latitude')], centers = 4)

# Add the cluster assignment as a new categorical variable
cahouses$GeoCluster <- as.factor(kmeans_result$cluster)

# Summarize the clustering result
table(cahouses$GeoCluster)

# Plot the geographic clusters
ggplot(cahouses, aes(x = longitude, y = latitude, color = GeoCluster)) +
  geom_point(alpha = 0.4) +
  labs(title = "K-Means Clustering of Longitude and Latitude", 
       x = "Longitude", y = "Latitude") +
  theme_minimal()
```

Now, GeoCluster is used as a categorical variable in place of longitude and latitude. This transformation captures geographic patterns and improves interpretability. Longitude and latitude are dropped from further analysis.


### Ocean proximity

OceanProximity describes whether the house is near the ocean. This is a critical factor when considering housing prices due to its strong association with real estate value. 
 
Histograms for median house value across different levels of OceanProximity are plotted to visually investigate for any meaningful distributional differences.

```{r ocean proximity, echo = FALSE}

nearbay <- filter(cahouses, ocean_proximity == "NEAR BAY")
hourfromocean <- filter(cahouses, ocean_proximity == "<1H OCEAN")
inland <- filter(cahouses, ocean_proximity == "INLAND")
nearocean <- filter(cahouses, ocean_proximity == "NEAR OCEAN")
island <- filter(cahouses, ocean_proximity == "ISLAND")

par(mfrow = c(2,2))
hist(nearbay$median_house_value, xlab = "Median House Value", main = "NEAR BAY" )

hist(hourfromocean$median_house_value, xlab = "Median House Value", main = "<1H FROM OCEAN", breaks = 20)

hist(inland$median_house_value, xlab = "Median House Value", main = "INLAND" )

hist(nearocean$median_house_value, xlab = "Median House Value", main = "NEAR OCEAN" )
```

```{r island, fig.width=4, fig.height=2}
par(mfrow = c(1,1))

hist(island$median_house_value, xlab = "Median House Value", main = "ISLAND" )

```

Upon reviewing these histograms, they all display a distinct right skew. To ensure that the different factor levels of OceanProximity are statistically significant from one another, ANOVA is run.  

```{r anova oceanproximity, collapse = FALSE}

anova_model <- aov(median_house_value ~ ocean_proximity, data = cahouses)
summary(anova_model)

```

The Anova resulted in F(4, 20428) = 1595, p < 0.001. This means that at least one factor level differs from the others. As a result, OceanProximity is still relevant to the study at hand. We'll perform post-hoc analyses later when the multiple regression model is built. 


### Chi-Squared Test between GeoCluster and OceanProximity

To ensure that the GeoCluster and OceanProximity variables are not confounding each other, a Chi-Squared Test of Independence is run. This test will check if there is an association between the two categorical variables. If the p-value is significant, we will need to assess the impact of using both variables in the model.

```{r chisq ToI, collapse = FALSE}

geo_ocean_table <- table(cahouses$GeoCluster, cahouses$ocean_proximity)
chisq.test(geo_ocean_table)

```

Since Χ^2(12) = 9684.5, p < 0.01 from the chi-squared test of independence, there appears to be an association between geographic cluster and ocean proximity. This concern will be addressed when candidate models are built. 


## Socioeconomic Factors 

Socioeconomic factors such as population distribution, economic status, and household distributions can significantly influence median house values and will be considered as part of this study. 

### Median Income

A histogram of median income distribution of the 20433 observations is created. 

```{r hist of med_income}
# income distribution
hist(cahouses$median_income, xlab = "Median Income", main = "Median Income Distribution")

```

In the distribution for median income, each bar represents an increment of 10,000. The first bar shows that roughly 100 block groups have a median income between 0 and 9,999. The distribution is right-skewed, with fewer than 500 block groups showing a median income of 100,000 or more.

However, we decided not to discretize median income further, as binning can reduce statistical power and cause information loss. Furthermore, the maximum median income observed was 150,001. The fact that exactly 50 block groups have this value suggests that incomes over 150,000 have already been binned in the original dataset.

```{r not binning median income, echo = TRUE}
#proof income was pre-discretized

  sum(cahouses$median_income == 150001)

  tail(sort(cahouses$median_income), 50)

```


### Inhabitant Characteristics 

The distributions for population and households are plotted below. 

```{r plot pop/household, fig.width=8, fig.height=4}

#Population distribution
par(mfrow = c(1,2))
hist(cahouses$population, xlab = "Population Count", xlim = c(0, 8000), main = "Population Distribution" ) 
text(x = 5000, y = 15000, labels = "60 Observations >= 8000 ", col = "red", cex = 0.8, pos = 1)
text(x = 5000, y = 14000, labels = "are cut off", col = "red", cex = 0.8, pos = 1)

sum(cahouses$population >= 8000)

#Household distribution
hist(cahouses$households, xlab = "Household Count", xlim = c(0, 3000), main = "Household Distribution" )
text(x = 1800, y = 11000, labels = "47 Observations >= 3000 ", col = "red", cex = 0.8, pos = 1)
text(x = 1800, y = 10000, labels = "are cut off", col = "red", cex = 0.8, pos = 1)

sum(cahouses$households >= 3000)

```

The histograms for population and household counts display right-skewed distributions, with a high frequency of block groups with smaller populations and household counts. Population-dense areas, typically urban or suburban, contribute to this skew by comprising smaller geographic areas. This leads to a concentration of block groups with smaller populations and household sizes. Consequently, it is likely that a transformation will be required to correct these skews before proceeding with the analysis.

## House Characteristics

### Median House Age

The distribution of housing median age is shown below.

```{r plot house med age}

# house age distribution
hist(cahouses$housing_median_age, xlab = "Age of house", main = "Median House Age Distribution" )


sum(cahouses$housing_median_age == 52)


```

The distribution for housing median age is approximately normal, except for the last bar, which represents a large spike at 52 years old. This suggests that the data for housing age may have been discretized or capped at 52 years, as 1,265 observations are recorded at this exact age. Nonetheless, since normality is largely satisfied, no transformation is required for housing age.

### Rooms and Bedrooms

Next, the distributions of total rooms and total bedrooms per block group are displayed.

```{r plot total rooms/bedrooms, fig.width=8, fig.height=4}

# Total room distribution
par(mfrow = c(1,2))
hist(cahouses$total_rooms, xlab = "Room Count", xlim = c(0,15000), main = "Total Rooms Distribution" )
text(x = 10000, y = 7500, labels = "97 Observations >= 15000 ", col = "red", cex = 0.8, pos = 1)
text(x = 10000, y = 6800, labels = "are cut off", col = "red", cex = 0.8, pos = 1)

sum(cahouses$total_rooms >= 15000)


# Total bedroom distribution
hist(cahouses$total_bedrooms, xlab = "Bedroom Count", xlim = c(0, 3000), main = "Total Bedrooms Distribution" )
text(x = 2000, y = 10000, labels = "67 Observations >= 3000 ", col = "red", cex = 0.8, pos = 1)
text(x = 2000, y = 9100, labels = "are cut off", col = "red", cex = 0.8, pos = 1)

sum(cahouses$total_bedrooms >= 3000)

```

Both the total rooms and total bedrooms distributions exhibit heavy right-skewed patterns. This is expected, as apartment complexes or smaller homes in densely populated areas tend to contribute lower counts of rooms and bedrooms per block group. Given the skewness, a transformation will likely be required to normalize these variables and improve the model's performance.


## Multicollinearity and Variable Selection

Multicollinearity occurs when two or more predictor variables in a model are highly correlated, which can obscure the true relationship between each predictor and the response variable. A key step before building the regression model is to check for multicollinearity among the numerical variables.

### Pairwise Scatterplot

A pairwise scatterplot is used to visually inspect the relationships between numerical variables and check for highly correlated pairs. The correlation coefficients between the variables are shown on the plot, and higher correlations indicate potential issues with multicollinearity.

```{r multicollinearity}

pairs.panels(cahouses[, -c(1, 2, 9, 10, 11)], pch=21, main="Pair-wise Scatter Plot of Numerical Variables")


```

Visually, the pairwise scatterplot reveals a few concerning cases of collinearity. For instance, the correlation between the number of rooms and the number of bedrooms is extremely high (0.98), suggesting these two variables are capturing very similar information. Other large correlations exist between population and households (0.91), and rooms with households (0.92). These strong correlations indicate multicollinearity, which may distort our regression results. In the next section, we formally test for multicollinearity using the Variance Inflation Factor (VIF).

### Variance Inflation Factor (VIF) Check

To formally detect multicollinearity, Variance Inflation Factors (VIF) are calculated for the predictors in the initial model. A VIF value greater than 10 typically indicates a high level of multicollinearity, which could affect the stability of the regression coefficients. However, it is important to note that for categorical variables transformed into dummy variables, high VIFs may not always be a cause for concern due to how reference categories are selected.

```{r vif check}

full_model <- lm(median_house_value~. - longitude - latitude, data = cahouses)
vif(full_model)


```

In examining the VIF values, Total Rooms (VIF = 12.97), Total Bedrooms (VIF = 36.05), and Households (VIF = 34.57) exhibit extremely high multicollinearity, which is consistent with our earlier visual inspection of the pairwise scatterplots. These high VIF values suggest that these variables provide overlapping information, and it may be beneficial to consider reducing the number of highly correlated variables to improve the model.

However, it is worth noting that GeoCluster and OceanProximity, which are categorical variables, have moderate VIF values that are not as concerning. High VIFs for dummy variables often arise due to small sample sizes in reference categories. In this case, while the VIF for OceanProximity is 2.19 and GeoCluster is 1.87, these values are not large enough to warrant concern, especially given that the categorical variables are dummy coded.

We will continue to address these multicollinearity issues in our variable selection and modeling steps. Specifically, it may be necessary to drop or transform certain numerical variables (such as Total Bedrooms and Households) to improve model stability, while recognizing that categorical variables may inherently have higher VIF values without seriously affecting the overall model fit.


# Regression Modeling

After performing the exploratory data analysis and preparing the data, the next step is to develop a regression model. The aim is to build a model that not only meets statistical requirements, such as satisfying assumptions about linearity, normality, and homoscedasticity, but also provides valuable practical insights for decision-making.

Categorical variables with more than 2 factor levels results in the creation of dummy variables. This means the coefficients for the dummy variables compare each category to their respective baseline. Our two categorical variables, Ocean_proximity and GeoCluster, have baselines of <1 hour from ocean and GeoCluster1 respectively. 

For instance, if ocean_proximityINLAND has a negative coefficient, it signifies that homes that are inland have lower median value compared to those within <1 hour from the ocean; this suggests that coastal proximity dramatically increases property value. Similarly, GeoCluster2 having a positive coefficient indicates that that region has higher median house values compared to GeoCluster1, implying those regions are more desirable or have higher property demand.

## Full Model 

We begin by fitting a full model with all predictors (excluding longitude and latitude, which were previously transformed into geographic clusters). This model will serve as the baseline for comparison.

```{r full model}

full_model <- lm(median_house_value ~ . - longitude - latitude, data = cahouses)

kable(round(summary(full_model)$coef, 3), caption = "Coefficient Matrix for Full Linear Model") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  column_spec(1:5, width = "4cm")



```

### Full Model Coefficient Interpretation

The coefficients below summarize the relationship between predictor variables and median house value:

- **Housing Median Age**: Each additional year of housing median age increases house value by $1,135, likely due to the appeal of established areas.

- **Total Rooms**: For each additional room, house value decreases by $6.72. This appears to be paradoxical, but it is likely the result of the collinearity between Total Rooms and Total Bedrooms. 

- **Total Bedrooms**: Each additional bedroom increases house value by $79.90, reflecting that bedrooms are more valuable than overall room count.

- **Population**: For each additional person in the population, house value decreases by $37.60. More populated areas tend to have lower house values, possibly due to crowding or lower space per household.

- **Households**: Each additional household increases house value by $73.83, suggesting that higher demand for housing raises prices.

- **Median Income**: For every 1 dollar increase in median income, house value increases by 3.95 dollars. Higher-income areas see higher home values.

- **Ocean Proximity**: The dummy variable represents the proximity of houses to the ocean.
  - **INLAND**: Inland properties are associated with a $66,400 decrease in value compared to properties within <1 hour from the ocean.
  - **ISLAND**: Island properties are associated with a $73,164 increase in value compared to properties within <1 hour from the ocean.
  - **NEAR BAY**: Properties near the bay are associated with a $4,257 decrease in value compared to those <1 hour from the ocean.
  - **NEAR OCEAN**: Properties near the ocean see a $13,969 increase in value compared to those <1 hour from the ocean.

- **GeoCluster**: The GeoCluster categorical variable captures geographic patterns and their relationship to house value.
  - **GeoCluster2**: Properties in this cluster are associated with a $22,843 increase in value compared to GeoCluster1.
  - **GeoCluster3**: Properties in this cluster see a $13,191 increase in value compared to GeoCluster1.
  - **GeoCluster4**: Properties in this cluster have a $13,359 decrease in value compared to GeoCluster1.


### Full Model Residual Analysis

To assess the model, we check for any violations of regression assumptions by examining residual plots:

```{r full model RA}
par(mfrow = c(2,2))
plot(full_model)

```

- **Residuals vs Fitted**: There’s a clear non-linearity, especially for higher fitted values. The relationship between predictors and house values may not be fully captured by a linear model.

- **Normal Q-Q**: Deviations from normality in the tails suggest that some residuals, particularly from influential points like observations 15361 and 18502, are outliers. This affects the model's validity for hypothesis testing.

- **Scale-Location**: The increasing spread of residuals with higher fitted values suggests heteroscedasticity, indicating non-constant variance. A transformation of the response variable could help address this.

- **Residuals vs Leverage**: Influential points such as 15361 and 18502 have high leverage and may be disproportionately impacting the model. These points require further investigation, and removing them could provide more stable results.

To address these violations, transformations such as a Box-Cox or log transformation of the response variable should be considered. Additionally, handling influential points and reducing multicollinearity can improve model stability and predictive performance.


## Reduced Model

The primary goal of developing a reduced model is to alleviate the multicollinearity present in the full model, as identified through the high VIF values for certain predictors. Reducing multicollinearity ensures more stable and interpretable regression coefficients, leading to a model that balances simplicity and predictive performance.

Additionally, we aim to create a parsimonious model, which is a model that explains the data with the fewest possible variables without sacrificing explanatory power. This is important because a simpler model is easier to interpret and often avoids overfitting, while still capturing the significant relationships between predictors and house values.

To address multicollinearity, we will iteratively remove the following variables:

- **Total Rooms**: Has high VIF and could be collinear with Total Bedrooms. Total Bedrooms typically is a better indicator of house value.
- **Population**: Another variable with potential collinearity with Households.
- **Households**: Has collinearity with population and total rooms.
- **A combination of the three**: The likely reduced model with low multicollinearity is one where multiple variables are taken out.

We will then compare the goodness-of-fit measures to determine which reduced model performs best.


### Reduced Model Criterion

We create three reduced models by removing each of the identified variables.

```{r reduced models created}

# Reduced Model 1: Remove Total Rooms
r_model1 <- lm(median_house_value ~ . - total_rooms - longitude - latitude, data = cahouses)

# Reduced Model 2: Remove Population
r_model2 <- lm(median_house_value ~ . - population - longitude - latitude, data = cahouses)

# Reduced Model 3: Remove Both Total Rooms and Population
r_model3 <- lm(median_house_value ~ . - total_rooms - population - longitude - latitude, data = cahouses)

# Reduced Model 4: All 3 variables
r_model4 <- lm(median_house_value ~ . - total_rooms - population - households - longitude - latitude, data = cahouses)

```

Next, we compare the goodness-of-fit metrics across the models to identify which one offers the best balance of model fit and simplicity.

```{r reduced GoF}

select=function(m){ # m is an object: model
 e = m$resid                           # residuals
 n0 = length(e)                        # sample size
 SSE=(m$df)*(summary(m)$sigma)^2       # sum of squared error
 R.sq=summary(m)$r.squared             # Coefficient of determination: R square!
 R.adj=summary(m)$adj.r                # Adjusted R square
 MSE=(summary(m)$sigma)^2              # square error
 Cp=(SSE/MSE)-(n0-2*(n0-m$df))         # Mellow's p
 AIC=n0*log(SSE)-n0*log(n0)+2*(n0-m$df)          # Akaike information criterion
 SBC=n0*log(SSE)-n0*log(n0)+(log(n0))*(n0-m$df)  # Schwarz Bayesian Information criterion
 X=model.matrix(m)                     # design matrix of the model
 H=X%*%solve(t(X)%*%X)%*%t(X)          # hat matrix
 d=e/(1-diag(H))
 PRESS=t(d)%*%d   # predicted residual error sum of squares (PRESS)- a cross-validation measure
 tbl = as.data.frame(cbind(SSE=SSE, R.sq=R.sq, R.adj = R.adj, Cp = Cp, AIC = AIC, SBC = SBC, PRD = PRESS))
 names(tbl)=c("SSE", "R.sq", "R.adj", "Cp", "AIC", "SBC", "PRESS")
 tbl = round(tbl, 2)
 tbl
 }


## Edited this because the original kable was difficult to read.

output <- rbind(select(r_model1), select(r_model2), select(r_model3), select(r_model4))

row.names(output) <- c("Total Rooms Removed", "Population Removed", "Total Rooms and Population Removed", "All 3 Removed")

# kable(output, caption = "Goodness-of-fit Measures of Candidate Models")

kable(round(output, 3), caption = "Goodness-of-fit Measures of Candidate Models") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  column_spec(1:5, width = "4cm")

vif(r_model1)

vif(r_model2)

vif(r_model3)

vif(r_model4)


###
```

The VIF values are still really large, I'm not quite sure I can manually reduce in a manner that gives a quality candidate model. For now, we'll move to automatic variable selection to identify our reduced models.


## Stepwise AIC

We begin by applying Stepwise AIC selection to reduce the model's complexity and improve interpretability. StepAIC minimizes the Akaike Information Criterion (AIC), which balances model fit and complexity. This method iteratively adds or removes variables, choosing the model with the lowest AIC.


```{r stepaic}

# Fit the full model
full_model <- lm(median_house_value ~ . - longitude - latitude, data = cahouses)

# Apply stepwise selection based on AIC
step_model <- stepAIC(full_model, direction = "both", trace = FALSE)

# Check the summary of the stepwise-selected model
kable(round(summary(step_model)$coef, 3), caption = "Coefficient Matrix for StepAIC-Selected Model") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  column_spec(1:5, width = "4cm")

```

The stepwise regression model removed none of the variables, making it identical to the full model. This is because the full model must have the lowest AIC already. However, because this model still suffers from multicollinearity, we will proceed to a different method for automatic variable selection.


## LASSO Regression

Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique that penalizes the absolute size of regression coefficients, shrinking less important coefficients to zero and effectively performing variable selection. This method is particularly useful when dealing with multicollinearity, and it helps create simpler, more interpretable models by automatically selecting the most relevant predictors.


We apply Lasso Regression to identify the most important variables and shrink the less relevant ones. The final model will only include predictors that contribute meaningfully to predicting median house value.


```{r lasso reg}

library(glmnet)

# Prepare the data for glmnet (requires matrix for predictors)
X <- model.matrix(median_house_value ~ . - longitude - latitude, cahouses)[,-1]
Y <- cahouses$median_house_value

# Fit Lasso model with cross-validation to find the best lambda
lasso_model <- cv.glmnet(X, Y, alpha = 1)

# Plot the cross-validated mean squared error for different values of lambda
plot(lasso_model)

# Get the best lambda that minimizes the cross-validated error
best_lambda <- lasso_model$lambda.min

# Fit the final Lasso model using the best lambda
final_lasso <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Extract the coefficients of the selected variables
lasso_coefs <- coef(final_lasso)
lasso_df <- as.data.frame(as.matrix(lasso_coefs))  # Convert sparse matrix to dataframe
colnames(lasso_df) <- c("Coefficient")

# Display coefficients in a table
kable(round(lasso_df, 3), caption = "Coefficient Matrix for Lasso-Selected Model") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  column_spec(1:2, width = "4cm")


```


### No variables removed by LASSO

The Lasso regression did not remove any variables, which indicates that all the predictors in the model have some level of significance in explaining the variance in house prices. This outcome suggests that despite the multicollinearity observed earlier, each predictor contributes meaningfully to the model. However, while Lasso didn’t eliminate variables, it did shrink their coefficients, which has important implications for the model's interpretation and performance.

By shrinking the magnitude of the coefficients, Lasso helps mitigate the overfitting that can arise when certain predictors have an overly strong influence on the model. In the full model, some predictors had relatively large coefficients, which could result in an unstable model, particularly when new data are introduced. The reduced coefficient values in the Lasso model indicate that while these predictors still contribute to house price predictions, their individual impact is more tempered, which helps improve the generalizability of the model to unseen data.

The fact that Lasso did not remove any predictors also implies that multicollinearity is still present in the model. However, by shrinking the coefficients, Lasso reduces the sensitivity of the model to multicollinearity. While this doesn't completely resolve the issue of multicollinearity, it makes the model less prone to the erratic changes in coefficients that can occur when collinear predictors are included.

In practice, this means that while the Lasso model is likely to generalize better to new data than the full model, it still contains all of the original predictors, which could complicate interpretation. Each predictor still plays a role in explaining house prices, but their contributions are reduced, resulting in a more stable, but slightly less interpretable, model. The implications of this are twofold: Lasso provides a safeguard against overfitting while still allowing for the full complexity of the data to be captured. However, if the goal is to simplify the model further, other strategies—such as variable interactions or further domain-driven selection—may be needed.

Overall, the Lasso model strikes a balance between retaining predictor significance and ensuring a more robust, generalizable model. By reducing the influence of individual variables without eliminating them, Lasso maintains the explanatory power of the model while making it more resistant to overfitting.


### LASSO Coefficient Interpretation

The coefficients above summarize the relationship between predictor variables and median house value after Lasso regularization:

- **Housing Median Age**: Each additional year of housing median age increases house value by $1,128, slightly reduced from the full model. This implies that the age of the house remains a strong positive factor in house valuation.

- **Total Rooms**: For each additional room, house value decreases by \$5.85, compared to $6.72 in the full model. This reduction suggests that Lasso minimized the influence of total rooms but did not remove it entirely.

- **Total Bedrooms**: Each additional bedroom increases house value by \$75.15, slightly smaller than in the full model (\$79.90), indicating that bedrooms are still valued more than overall room count but with reduced impact.

- **Population**: For each additional person in the population, house value decreases by \$37.41, nearly identical to the full model. More populated areas still tend to have lower house values.

- **Households**: Each additional household increases house value by \$73.64, again similar to the full model. This continues to suggest that higher demand for housing raises property prices.

- **Median Income**: For every 1 dollar increase in median income, house value increases by $3.92, which is consistent with the full model. Higher-income areas still correspond to higher house prices.

- **Ocean Proximity**:

  - **INLAND**: Inland properties are associated with a $66,688 decrease in value compared to properties within <1 hour from the ocean.
  - **ISLAND**: Island properties are associated with a $169,482 increase in value compared to properties within <1 hour from the ocean.
  - **NEAR BAY**: Properties near the bay are associated with a $3,716 decrease in value compared to those <1 hour from the ocean.
  - **NEAR OCEAN**: Properties near the ocean are associated with a $13,553 increase in value compared to those <1 hour from the ocean.
  
- **GeoCluster**:
  - **GeoCluster2**: Properties in this cluster are associated with a $22,153 increase in value compared to GeoCluster1.
  - **GeoCluster3**: Properties in this cluster see a $12,754 increase in value compared to GeoCluster1.
  - **GeoCluster4**: Properties in this cluster have a $13,715 decrease in value compared to GeoCluster1.


### LASSO Residual Analysis

We now assess the Lasso model by checking for any violations of regression assumptions through residual plots:

```{r lasso RA}

# Predict values using the Lasso model
yhat_lasso <- predict(final_lasso, newx = X)

# Calculate residuals
residuals_lasso <- Y - yhat_lasso

# Plot residuals against predicted values
par(mfrow = c(2, 2))

# Residuals vs Fitted (Predicted) values
plot(yhat_lasso, residuals_lasso, 
     main = "Residuals vs Fitted", 
     xlab = "Fitted values (Predicted)", 
     ylab = "Residuals")
abline(h = 0, col = "red")

# Q-Q plot for normality
qqnorm(residuals_lasso, main = "Normal Q-Q Plot")
qqline(residuals_lasso, col = "red")

# Scale-location plot (for homoscedasticity)
sqrt_residuals <- sqrt(abs(residuals_lasso))
plot(yhat_lasso, sqrt_residuals, 
     main = "Scale-Location", 
     xlab = "Fitted values (Predicted)", 
     ylab = "Sqrt(|Residuals|)")
abline(h = 0, col = "red")

# Residuals vs Leverage plot
# For this plot, we need the hat matrix (leverage) values
hat_values <- hatvalues(lm(Y ~ X))  # Approximate hat values
plot(hat_values, residuals_lasso, 
     main = "Residuals vs Leverage", 
     xlab = "Leverage", 
     ylab = "Residuals")
abline(h = 0, col = "red")

par(mfrow = c(1, 1))  # Reset to default plotting window



```

- **Residuals vs Fitted**: A slight curvature suggests some non-linearity, especially for higher fitted values. However, given the large sample size, the model still captures the overall trend effectively, and minor deviations are not a major concern.

- **Normal Q-Q Plot**: The tails deviate from normality, reflecting a few outliers. With a large dataset, small deviations from normality are expected and don’t severely impact the model’s overall validity.

- **Scale-Location Plot**: There is some heteroscedasticity (increasing spread of residuals), but the large sample size mitigates its impact on inference, ensuring parameter estimates remain reliable.

- **Residuals vs Leverage Plot**: A few high-leverage points are observed, but they have minimal impact due to the large number of observations, reducing the overall influence of these outliers.

In summary, while minor assumption violations exist, the large sample size helps mitigate their effects, allowing the model to perform well overall.






