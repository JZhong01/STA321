---
title: "Case Study: Predictive Model for Breast Cancer "
author: "Joshua Zhong"
date: "01/06/2024"
output:
 html_document:
  toc: yes
  toc_depth: 4
  toc_float: yes
  fig_width: 6
  fig_caption: yes
  number_sections: yes
  theme: readable
  editor_options:
  chunk_output_type: console
---


```{=html}

<style type="text/css">

/* Cascading Style Sheets (CSS) is a stylesheet language used to describe the presentation of a document written in HTML or XML. it is a simple mechanism for adding style (e.g., fonts, colors, spacing) to Web documents. */

h1.title {  /* Title - font specifications of the report title */
  font-size: 24px;
  color: DarkRed;
  text-align: center;
  font-family: "Gill Sans", sans-serif;
}
h4.author { /* Header 4 - font specifications for authors  */
  font-size: 20px;
  font-family: system-ui;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - font specifications for the date  */
  font-size: 18px;
  font-family: system-ui;
  color: DarkBlue;
  text-align: center;
}
h1 { /* Header 1 - font specifications for level 1 section title  */
    font-size: 22px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}
h2 { /* Header 2 - font specifications for level 2 section title */
    font-size: 20px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { /* Header 3 - font specifications of level 3 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - font specifications of level 4 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

body { background-color:white; }

.highlightme { background-color:yellow; }

p { background-color:white; }

</style>
```

```{r setup, include=FALSE}

   breast_cancer_data = read.csv("https://raw.githubusercontent.com/JZhong01/STA321/main/Topic%204%20(Multiple%20Logistic%20Regression/syntheticBreastCancerData.csv", 
   header = TRUE)
   
  options(scipen = 2)

   library(knitr)
   library(leaflet)
   library(EnvStats)
   library(MASS)
   library(phytools)
   library(boot)
   library(psych)
   library(car)
   library(dplyr)
   library(kableExtra)

# Specifications of outputs of code in code chunks
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	comment = FALSE,
	results = TRUE, 
	digits = 4
)
   
```


# Data set Description

The data set was synthetic data set created for practice purposes in the Book *Applied Analytics through Case Studies Using SAS and R* by Deepti Gupta and APress. It has 600 observations with 11 total variables - 10 numerical variables and 1 categorical variable. Only 10 of these variables are predictors, however, because observation number is included as a numerical, but has no predictive power. 

The link to the raw CSV file in posted on GitHub:  https://raw.githubusercontent.com/JZhong01/STA321/main/Topic%204%20(Multiple%20Logistic%20Regression/syntheticBreastCancerData.csv.

- Sample Number: A numeric variable that's a unique identifier for each sample. Not used in the model as it has no predictive power.  
- Thickness of Clump: A numeric variable that indicates thickness of cell clump. Benign cells are typically monolayered, while cancerous cells tend to form multilayers.
- Cell Size Uniformity: Numeric variable that measures uniformity of cell size. Benign cells generally vary in size less than malignant cells. 
- Cell Shape Uniformity: Numeric variable that assesses the uniformity of cell shape. Benign cells usually exhibit consistent shapes, while malignant cells display a wide range of shapes.
- Marginal Adhesion: Numeric variable that reflects how cells stick together. Benign cells tend to adhere more strongly to each other, whereas malignant cells often demonstrate loose or minimal adhesion.
- Single Epithelial Cell Size: Numeric variable that evaluates the size of epithelial cells. In benign cases, these cells are typically of a normal size, whereas in malignant cases, they are significantly enlarged. 
- Bare Nuclei: Numeric variable that describes the bare nuclei's interaction with cytoplasm. In benign cells, bare nuclei are usually not surrounded by cytoplasm, contrasting with cancer cells, where they are often surrounded by cytoplasm.
- Bland Chromatin: Numeric variable that describes the appearance of chromatin within the nucleus. Benign cells are characterized by uniform or fine chromatin, while cancer cells usually have coarse chromatin.
- Normal Nucleoli: Numeric variable that indicates the prominence of nucleoli within the cells. In benign cells, nucleoli are typically very small, but in malignant cells, they are more prominent. 
- Mitoses: Numeric variable that relates to the rate of cell division. Benign cells display normal mitotic activity, whereas malignant cells often exhibit abnormal or increased cell growth.
- Outcome: Categorical response variable that represents the diagnosis, with "No" indicating the presence of benign cells and "Yes" indicating the presence of malignant breast cancer.


# Research Question

The primary objective of this analysis is to identify if these 9 cellular characteristics are factors that can predict the outcome (benign or malignant) of breast cancer cases.  

The objectives of this case study are as follows: 

- Develop a predictive model to identify whether the observed cellular characteristics can effectively predict the outcome of breast cancer cases as benign or malignant.
- Determine the importance of each cellular characteristic in predicting breast cancer outcomes. Identify which features contribute significantly to the predictive power of the model.
- Evaluate the performance of the predictive model using appropriate metrics. Assess the model's ability to correctly classify both benign and malignant cases.
- Implement cross-validation techniques to ensure the model's robustness and generalizability. 

The hypotheses of the study are that: 

- These cellular characteristics are predictive of breast cancer outcomes, and there exists a significnat relationship between certain factors and the presence of malignant breast cancer. 

- The predictive model's accuracy is significantly better than random chance, indicating its efficacy in identifying breast cancer outcomes. 


# Exploratory Analysis

In our case study, we analyze the synthetic_cancer_data data set, which includes critical cellular features to differentiate between benign and malignant breast cancer. Key variables such as 'Thickness of Clump', 'Cell Size Uniformity', and 'Cell Shape Uniformity' provide insights into the physical attributes of cancer cells. Additional factors like 'Marginal Adhesion', 'Bland Chromatin', and 'Mitoses' help in understanding cellular behaviors and division patterns. This comprehensive analysis aims to enhance the accuracy of breast cancer diagnosis through detailed examination of these cellular characteristics.


## Modify Response


When performing logistic regression, it's a standard practice to code the response variable as binary numerical values. In our data set, 'Outcome' will be recoded such that "Yes" represents 1 and "No" represents 0. Here, '1' signifies 'Success' in identifying the presence of cancer, while '0' indicates 'Failure' to do so, within the context of statistical modeling. This terminology does not reflect any value judgment about the disease itself; rather, it's a convention in statistical analysis to facilitate the computational process of the logistic regression.



```{r recode response}

breast_cancer_data$Outcome <- ifelse(breast_cancer_data$Outcome == 'Yes', 1, 0)


```


## Data Management

This synthetic data set is checked for any missing variables, outliers, and anything else that may require transformation. 

### Checking for NA values

We first ensure that there are no missing values in our data set and observations. 

```{r check NAs}

sum(is.na(breast_cancer_data$Thickness_of_Clump))
sum(is.na(breast_cancer_data$Cell_Size_Uniformity))
sum(is.na(breast_cancer_data$Cell_Shape_Uniformity))
sum(is.na(breast_cancer_data$Marginal_Adhesion))
sum(is.na(breast_cancer_data$Single_Epithelial_Cell_Size))
sum(is.na(breast_cancer_data$Bare_Nuclei))
sum(is.na(breast_cancer_data$Bland_Chromatin))
sum(is.na(breast_cancer_data$Normal_Nucleoli))
sum(is.na(breast_cancer_data$Outcome))

```

There are no missing values in our data set. This is unsurprising because the data was fabricated for logistic regression practice. However, we must check this regardless because no assumptions can be made. 

### Check for Outliers

We try to ascertain the number of outliers found in our data. We do so by calculating the interquartile range (IQR) which is the range from the 75th percentile and the 25th percentile. Using a commonly used rule-of-thumb where outliers are values that are 1.5 * IQR above the 75th percentile and 1.5 * IQR below the 25th percentile. 

```{r check outliers}

#remove response + observation ID variables 
predictor_data <- breast_cancer_data[,-c(1,11)]


#Calculate Q1, Q3, IQR
Q1 <- apply(predictor_data, 2, quantile, probs=0.25)
Q3 <- apply(predictor_data, 2, quantile, probs=0.75)
IQR <- Q3 - Q1


# Identifying outliers
outliers <- apply(predictor_data, 2, function(x) {
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  x < lower_bound | x > upper_bound
})

outliers_per_variable <- colSums(outliers)

```

Using the measure, we found many outliers based off each predictor variable individual, with the largest upwards of 85 "outliers". However, this isn't a reliable measure of the nature of how many and how large the outliers in the data is. This is because there are many predictor variables which means that a 'lower than normal' value for Thickness of Clump could be explained by other predictor variables or indicate potential malignant cells. 

As a result, no transformations are conducted and no outliers were removed. These are 2 common methods of dealing with outliers, but this is an associative study so we're not concerned with creating a predictive model but merely analyzing the relationship between the predictor variables and the response or Outcome. 


## Variable Inspection

In multiple logistic regression, there aren't many diagnostic tools that we can utilize. However, this doesn't mean that pre-processing procedures aren't performed on predictor variables. 


### Pairwise Scatter Plot

Pairwise scatter plots are used to inspect the predictor variables. Any potential problems can be more easily identified with this plot. 

```{r pairwise scatter plots}

pairs.panels(breast_cancer_data[,-c(1,11)], 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )


```
<br>

The scatter plot exposes that many of the predictor variables exhibit unimodal but right skews. In addition, there is moderate correlation found between many of the variables.

### Discretizing Variables

In order to address the heavy right skews experienced by some of the predictor variables, we considered discretizing variables. Converting continuous data into discrete categories not only simplifies the data but also helps in managing skewed distributions, making the statistical analysis more robust, especially in models sensitive to non-normal distributions.


Let's take a look at all the numerical predictor variables for our model: 

```{r histogram of variables}

par(mfrow=c(1,3))

hist(predictor_data$Thickness_of_Clump, xlab = "Clump Thickness", main = "Thickness of Clump")
hist(predictor_data$Cell_Shape_Uniformity, xlab = "Shape Uniformity", main = "Cell Shape Uniformity")
hist(predictor_data$Cell_Size_Uniformity, xlab = "Size Uniformity", main = "Cell Size Uniformity")

hist(predictor_data$Marginal_Adhesion, xlab = "Margin", main = "Marginal Adhesion")
hist(predictor_data$Single_Epithelial_Cell_Size, xlab="Single cell size", main = "Single Epithelial Cell Size")
hist(predictor_data$Bare_Nuclei, xlab = "Nuclei", main = "Bare Nuclei")

hist(predictor_data$Bland_Chromatin, xlab = "Chromatin", main = "Bland Chromatin")
hist(predictor_data$Normal_Nucleoli, xlab="Nucleoli", main = "Normal Nucleoli")
hist(predictor_data$Mitoses, xlab = "Mitoses", main = "Mitoses")

```
<br>

Many of the individual predictors in our dataset exhibit right-skewed distributions, indicating a concentration of values towards the lower end with a tail extending towards higher values. Right-skewed distributions suggest that some observations have larger values, potentially influencing model predictions. 

To mitigate the skewness, discretizing these variables is considered. Discretization involves grouping continuous values into discrete intervals or categories. This process can be advantageous as it reduces the impact of extreme values and outliers, making the data more resilient to the influence of skewed distributions. 

However, in this case, discretizing may not be beneficial. Discretization can lead to information loss and oversimplification of the data, especially if the original continuous values provide valuable insights. Additionally, it might introduce artificial patterns or categories that do not reflect the true nature of the predictors. Therefore, in this analysis, retaining the original continuous nature of the predictors may be more appropriate for capturing the nuances in the data.  

### Log Transformation 

Considering the right-skewed distributions of individual predictors, log transformation is a viable approach to address the skewness. Log transformation involves taking the logarithm of the variable values, which tends to compress higher values and stretch lower values, effectively reducing the right skewness. This is particularly useful when dealing with variables that exhibit a wide range of values. 

However, in this analysis, the use of log transformation may not be necessary. The dataset contains a sample size of 600 observations, which is sufficiently large for the Central Limit Theorem (CLT) to come into play. The CLT states that the distribution of the sample mean of a sufficiently large sample approaches a normal distribution, even if the underlying population distribution is not normal. Given the sizable dataset, the impact of right-skewed distributions on the overall analysis may be mitigated by the CLT, making log transformation unnecessary for achieving a semi-normal distribution.


### Multicollinearity

The pairwise scatter plot we had earlier exhibited a heavy presence of correlated pairs of predictor variables. Since there are 9 numerical predictor variables, 36 pairwise comparisons were made. A staggering 16 of these pairs had a correlation coefficient of 0.60 or more. A correlation coefficient measures the strength and direction of the relationship between 2 variables, and having a coefficient of 0.60 indicates a moderate to strong linear relationship. Having so many pairs of predictors correlated with one another indicates multicollinearity. 

Multicollinearity is problematic as too much can obscure the individual effect of each predictor on the outcome variable, making it difficult to ascertain the true relationship between predictors and the response.  

Addressing multicollinearity in the model requires careful consideration of various techniques. One effective approach is the utilization of variable selection techniques, which involve iteratively adding and removing variables based on their impact on the model's fit. Another method to combat multicollinearity is employing regularization techniques like Ridge regression, which introduces a penalty to the coefficients of correlated variables, thereby reducing the magnitude of collinear predictors. Additionally, an alternative strategy involves combining correlated variables into a single predictor using principal component analysis. 

However, after careful consideration, it is concluded that standardizing the variables is the most suitable approach for addressing multicollinearity in this specific analysis. Standardization ensures that all variables are on a common scale, mitigating the impact of collinearity and facilitating more stable and interpretable regression coefficients. This choice aligns with the specific characteristics of the data set and the goals of the analysis. 

We will obtain candidate models after using step-wise variable selection upon our standardized predictors and then cross-validating the data to ascertain the best model.  


### Standardizing Numerical Predictor Variables. 

Standardizing numerical variables involves transforming them in a way that ensures they have a mean of 0 and a standard deviation of 1. This process is essential when dealing with variables measured on different scales, as it puts them on a common scale, preventing one variable from dominating others during model training. In this analysis, standardizing the numerical variables is critical for mitigating the impact of multicollinearity and ensuring that each predictor contributes fairly to the predictive model, improving stability and interpretability of regression coefficients.

```{r standardizing vars}

## standardizing numerical variables
breast_cancer_data$sd.clump_thickness = (breast_cancer_data$Thickness_of_Clump-mean(breast_cancer_data$Thickness_of_Clump))/sd(breast_cancer_data$Thickness_of_Clump)

breast_cancer_data$sd.size_uniformity = (breast_cancer_data$Cell_Size_Uniformity-mean(breast_cancer_data$Cell_Size_Uniformity))/sd(breast_cancer_data$Cell_Size_Uniformity)

breast_cancer_data$sd.shape_uniformity = (breast_cancer_data$Cell_Shape_Uniformity-mean(breast_cancer_data$Cell_Shape_Uniformity))/sd(breast_cancer_data$Cell_Shape_Uniformity)

breast_cancer_data$sd.marginal_adhesion = (breast_cancer_data$Marginal_Adhesion-mean(breast_cancer_data$Marginal_Adhesion))/sd(breast_cancer_data$Marginal_Adhesion)

breast_cancer_data$sd.single_ep_size = (breast_cancer_data$Single_Epithelial_Cell_Size-mean(breast_cancer_data$Single_Epithelial_Cell_Size))/sd(breast_cancer_data$Single_Epithelial_Cell_Size)

breast_cancer_data$sd.bare_nuclei = (breast_cancer_data$Bare_Nuclei-mean(breast_cancer_data$Bare_Nuclei))/sd(breast_cancer_data$Bare_Nuclei)

breast_cancer_data$sd.bland_chromatin = (breast_cancer_data$Bland_Chromatin-mean(breast_cancer_data$Bland_Chromatin))/sd(breast_cancer_data$Bland_Chromatin)

breast_cancer_data$sd.normal_nucleoli = (breast_cancer_data$Normal_Nucleoli-mean(breast_cancer_data$Normal_Nucleoli))/sd(breast_cancer_data$Normal_Nucleoli)

breast_cancer_data$sd.mitoses = (breast_cancer_data$Mitoses-mean(breast_cancer_data$Mitoses))/sd(breast_cancer_data$Mitoses)


## drop the original variables except for the response variable
sd.breast_cancer_data = breast_cancer_data[, -(1:10)]



```


# Regression Modeling


## Building Candidate Models

It's important that we build multiple models to compare with one another to assess for quality of fit. In this case study we will compare 3 models: full model without alterations, full model with standardized numerical predictors, and reduced model from step-wise regression. 

We will look at each of these models' summary statistics separately first to consider the impact of each variable. This is to help with model interpretability on top of our goal of having a model with predictive power. 

### Full Model without modifications

The first candidate model is a generalized linear model performed on all the unmodified predictors. 

```{r full model w/o modifications}

full_model = glm(Outcome~ Thickness_of_Clump+Cell_Size_Uniformity+Cell_Shape_Uniformity+Marginal_Adhesion+Single_Epithelial_Cell_Size+Bare_Nuclei+Bland_Chromatin+Normal_Nucleoli+Mitoses, 
          family = binomial(link = "logit"),  #  logit(p) = log(p/(1-p))!
          data = breast_cancer_data)  

kable(summary(full_model)$coef, 
      digits = 4, 
      scientific = TRUE,
      caption="Summary of inferential statistics of the full model")



```
<br>

The full model appears to have multiple variables that are insignificant. We will deal with this later when we use goodness-of-fit measures to select the best model. 


### Full model with Standardized Predictors

This model includes all the variables, only now the numerical predictors are standardized.

```{r full model w/ standardized predictors}

full_sdmodel = glm(Outcome~., 
          family = binomial(link = "logit"),  #  logit(p) = log(p/(1-p))!
          data = sd.breast_cancer_data) 

kable(summary(full_sdmodel)$coef, 
      digits = 4, 
      scientific = TRUE,
      caption="Summary of inferential statistics of the full model with standardized predictors")



```
<br>

Standardizing appeared to have no impact on the p-values of our predictors. This means that the statistical significance of individual predictors was not notably influenced by standardization in the context of our model. 

As a result, we reevaluate the multicollinearity in our new full model with standardized variables. 


```{r pairwise scatter plots for standardized}

pairs.panels(sd.breast_cancer_data[,-1], 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )


```

The multicollinearity among our standardized predictors remains high. As a result, we will employ automatic variable selection to control for this.



### Reduced Model from Stepwise Regression

Automatic step-wise regression was performed on both the untouched full model as well as the full model with standardized predictors. 

```{r stepwise reduced model}

reduced_model = stepAIC(full_model, 
                      direction = "both",   # stepwise selection
                      trace = 0   # do not show the details
                      )


kable(summary(reduced_model)$coef, digits = 4,
      caption="Summary of inferential statistics of reduced model")



reduced_sdmodel = stepAIC(full_sdmodel, 
                      direction = "both",   # stepwise selection
                      trace = 0   # do not show the details
                      )

kable(summary(reduced_sdmodel)$coef, digits = 4, 
      caption="Summary of inferential statistics of reduced model with standardized predictors")




```
<br>

Interestingly enough, it appears as though there appears to be no difference between standardized and not standardized predictors in explaining the model. Both models removed the same predictor variables Size Uniformity and Single Epithelial Cell Size. 

## Data Splitting

For our model development process, we randomly partition our data set into two distinct subsets: a training set comprising 70% of the data and a sample containing the remaining 30%. 

The training set serves as the foundation for the exploration, refinement, and selection of candidate models. Here, we conduct an extensive search for potential models, leveraging the training data to iteratively validate their performance and ultimately identify the most promising model using the cross-validation method. This comprehensive approach allows us to assess how well each candidate model generalizes to unseen data and aids in the avoidance of overfitting to the training set. 

Once the final model is identified through this iterative process, we subsequently assess its performance on the untouched sample, providing a reliable measure of the model's predictive capabilities on new independent data.

```{r split the data}

## splitting data: 70% training and 30% testing
n <- dim(sd.breast_cancer_data)[1]
train.n <- round(0.7*n)
train.id <- sample(1:n, train.n, replace = FALSE)
## training and testing data sets
train <- sd.breast_cancer_data[train.id, ]
test <- sd.breast_cancer_data[-train.id, ]

fulltrain <- breast_cancer_data[train.id,]
fulltest <- breast_cancer_data[-train.id,]


```


## Cross Validation for Model Identification

In our analysis, we implement a robust 5-fold cross-validation approach to rigorously assess the performance of candidate models developed on the training data. This process involves partitioning the training set into five subsets, with each subset taking turns as the validation set while the remaining four are used for model training. By repeating this procedure five times, we obtain a comprehensive evaluation of each model's generalization performance across different subsets of the training data. The average predictive errors are then calculated, guiding us in the selection of the most effective model. This meticulous cross-validation strategy ensures that our final model is not only optimized for the training set but also demonstrates strong predictive capabilities on new, unseen data.

```{r 5 fold cross validation}

## 5-fold cross-validation
k=5
## floor() function must be used to avoid producing NA in the subsequent results
fold.size = floor(dim(train)[1]/k)
## PE vectors for candidate models
PE1 = rep(0,5)
PE2 = rep(0,5)
PE3 = rep(0,5)
PE4 = rep(0,5)
for(i in 1:k){
  ## Training and testing folds
  valid.id = (fold.size*(i-1)+1):(fold.size*i)
  valid = train[valid.id, ]
  train.dat = train[-valid.id,]
  fullvalid = fulltrain[valid.id,]
  fulltrain.dat = fulltrain[-valid.id,]
  
##  full model untouched 
  candidate01 = glm(Outcome~Thickness_of_Clump + Cell_Size_Uniformity + Cell_Shape_Uniformity + Marginal_Adhesion + Single_Epithelial_Cell_Size + Bare_Nuclei + Bland_Chromatin + Normal_Nucleoli + Mitoses , family = binomial(link = "logit"),  
                    data = fulltrain.dat)  
## full model standardized
  candidate02 = glm(Outcome~., 
                    family = binomial(link = "logit"),  
                    data = train.dat) 
  
## reduced model untouched 
  candidate03 = glm(Outcome~Thickness_of_Clump + Cell_Shape_Uniformity + Marginal_Adhesion + Bare_Nuclei + Bland_Chromatin + Normal_Nucleoli + Mitoses , family = binomial(link = "logit"),  
                    data = fulltrain.dat)
  
## reduced model standardized
  candidate04 = glm(Outcome~. - sd.size_uniformity - sd.single_ep_size, 
                    family = binomial(link = "logit"),  
                    data = train.dat) 

  
   
   
  ##  predicted probabilities of each candidate model
   pred01 = predict(candidate01, newdata = fullvalid, type="response")
   pred02 = predict(candidate02, newdata = valid, type="response")
   pred03 = predict(candidate03, newdata = fullvalid, type="response")
   pred04 = predict(candidate04, newdata = valid, type = "response")
   
   pre.outcome01 = ifelse(pred01 > 0.5, 1, 0)
   pre.outcome02 = ifelse(pred02 > 0.5, 1, 0)
   pre.outcome03 = ifelse(pred03 > 0.5, 1, 0)
   pre.outcome04 = ifelse(pred04 > 0.5, 1, 0)
   
   PE1[i] <- sum(pre.outcome01 == fullvalid$Outcome) / length(fullvalid$Outcome)
   PE2[i] <- sum(pre.outcome02 == valid$Outcome) / length(valid$Outcome)
   PE3[i] <- sum(pre.outcome03 == fullvalid$Outcome) / length(fullvalid$Outcome)
   PE4[i] <- sum(pre.outcome04 == valid$Outcome) / length(valid$Outcome)
}

sum(PE1)
sum(PE2)
sum(PE3)
sum(PE4)


avg.pe = cbind(PE1 = mean(PE1), PE2 = mean(PE2), PE3 = mean(PE3), PE4 = mean(PE4))
kable(avg.pe, digits = 4, caption = "Average of prediction errors of candidate models")



``` 

## Model Selection

In the process of model selection through cross-validation, predictive errors played a crucial role in evaluating and comparing the performance of candidate models. The cutoff probability of 0.5 was utilized as a threshold to classify predicted outcomes as either positive or negative. Models were assessed based on the proportion of misclassifications, with instances where the predicted probability exceeded 0.5 being labeled positive and below 0.5 as negative. By calculating the predictive errors for each candidate model across multiple folds, an average error rate was derived, aiding in the identification of the most suitable model. This approach provided a quantitative metric for model performance, assisting in the final selection of the model with the lowest average predictive error.



# Final Model

The preceding cross-validation process determined the optimal model using a pre-defined cutoff of 0.5. However, to accurately report the model's performance to the client, it is essential to assess its accuracy on a withheld test dataset. Consequently, the final model's real accuracy is computed by predicting outcomes on the test data:



```{r actual accuracy}


pred03 = predict(candidate03, newdata = fulltest, type="response")
pred03.outcome = ifelse(as.vector(pred03)>0.5, 1, 0)

accuracy = sum(pred03.outcome == fulltest$Outcome)/length(pred03)
kable(accuracy, caption="The actual accuracy of the final model")


```
<br>

The actual accuracy of the final model, denoted by 'x', is reported in the above table as `r accuracy`. This value reflects the genuine performance of the selected model on the test data set, providing a more reliable measure of its accuracy. It is important to note that due to the utilization of a random split method for defining training and testing data, slight variations in performance metrics may occur when rerunning the code.


# Summary 


Our final model was derived from a synthetic data set consisting of 600 observations and 11 variables, with 9 predictors capturing various cellular characteristics indicative of benign or malignant breast cancer. We aimed to associate these cellular traits with cancer outcomes. 

There were many alterations we had to perform on the data. We began with recoding the categorical response variable 'Outcome' from "Yes" and "No" to 1 and 0, ensuring compatibility for logistic regression analysis. 
Preliminary checks for missing values and outliers followed, leading to the discovery of substantial multicollinearity, which we addressed through automatic variable selection using step-wise regression. We then split the data 70% for training and 30% for testing, using cross validation to find the model with the best predictive power. 

Both reduced models, with and without standardized predictor variables had the same predictive power, so we are using the simpler model where 

As a result, our final model explains that certain factors statistically significantly increase the odds that cancer (our 'success' value) is diagnosed, some showing upwards of increasing the odds by 60%. 


# References

Why is it bad to discretize a continuous variable? (2022, October 14). Cross Validated. Retrieved January 4, 2024, from https://stats.stackexchange.com/questions/592246/why-is-it-bad-to-discretize-a-continuous-variable

Gupta, D. (2018). *Applied Analytics through Case Studies Using SAS and R*. APress.

Ciaburro G. (2018). *Regression Analysis with R: Design and Develop Statistical Nodes to Identify Unique Relationships Within Data at Scale*. Packt Publishing. 



